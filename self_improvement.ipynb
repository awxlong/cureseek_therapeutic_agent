{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    MAX_TRAIN = 100\n",
    "    MAX_TOKENS = 2048\n",
    "    NUM_GENERATIONS = 4\n",
    "    USE_PEFT = True\n",
    "    BATCH_SIZE=1\n",
    "    MAX_STEPS = 80\n",
    "    \n",
    "    BETA = 0.04\n",
    "    LR = 1.e-5\n",
    "    \n",
    "    MODEL_NAME = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "    \n",
    "    step_count=10\n",
    "    DEBUG = False\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are an expert clinical reasoning agent specializing in pharmaceutical therapeutics. Your task is to analyze multiple-choice questions. First, provide a detailed step-by-step reasoning process within <think> tags, explaining why the correct option is right and the others are wrong. Then, state the final correct answer in the format 'Letter: Full Answer Text'.\"\"\"\n",
    "\n",
    "    DATA_FILE_PATH = \"/kaggle/input/curebench-training/curebench_validation_preprocessed.json\" \n",
    "    OUTPUT_DIR = \"/kaggle/working/cureseek-sft-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_PATH = \"model_ckpoints/results/cureseek-sft-v1/final_adapter\" # Path from the previous training script\n",
    "MERGED_MODEL_PATH = \"model_ckpoints/cureseek-sft-v1-merged\" # Where to save the new model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load Base Model and Tokenizer ---\n",
    "print(\"Loading base model...\")\n",
    "# Load the base model in a higher precision for merging.\n",
    "# bfloat16 is a good choice for performance and memory.\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    CFG.MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME, \n",
    "                                        #   trust_remote_code=True\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Load LoRA Adapter and Merge ---\n",
    "print(f\"Loading LoRA adapter from {ADAPTER_PATH}...\")\n",
    "# Load the PeftModel by combining the base model with the adapter\n",
    "model = PeftModel.from_pretrained(original_model, ADAPTER_PATH)\n",
    "\n",
    "print(\"Merging adapter weights with the base model...\")\n",
    "# This method merges the LoRA weights into the base model's weights\n",
    "merged_model = model.merge_and_unload()\n",
    "print(\"Merge complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Save the Merged Model ---\n",
    "# Now you have a standalone model that can be used without the PEFT library\n",
    "print(f\"Saving merged model to {MERGED_MODEL_PATH}...\")\n",
    "merged_model.save_pretrained(MERGED_MODEL_PATH)\n",
    "tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "print(\"Merged model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "MERGED_MODEL_PATH = \"./cureseek-sft-v1-merged\" # The model we just created\n",
    "ORIGINAL_DATA_FILE = \"./path/to/your/full_curebench_data.json\" # The ORIGINAL dataset with all questions\n",
    "TXAGENT_OUTPUT_FILE = \"./your_filtered_data.json\" # Your file with the \"correct_or_not\" flag\n",
    "\n",
    "# --- Load the Fine-Tuned Model and Tokenizer ---\n",
    "print(f\"Loading fine-tuned model from {MERGED_MODEL_PATH}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MERGED_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Ensure pad token is set for generation\n",
    "\n",
    "# --- Prepare the Data ---\n",
    "# We need to find the questions that TxAgent answered incorrectly.\n",
    "print(\"Preparing data for inference...\")\n",
    "original_df = pd.read_json(ORIGINAL_DATA_FILE, lines=True) # Assuming JSONL format\n",
    "txagent_df = pd.read_json(TXAGENT_OUTPUT_FILE, lines=True) # Assuming your filtered file is also JSONL\n",
    "\n",
    "# Merge to get the correct answer letter and the TxAgent correctness flag\n",
    "merged_df = pd.merge(original_df, txagent_df[['id', 'correct_or_not']], on='id', how='left')\n",
    "\n",
    "# Filter for questions that were answered incorrectly by TxAgent\n",
    "incorrect_df = merged_df[merged_df['correct_or_not'] == False].copy()\n",
    "print(f\"Found {len(incorrect_df)} questions that TxAgent answered incorrectly.\")\n",
    "\n",
    "# --- Define the Generation Prompt Template ---\n",
    "# This is the crucial part: we guide the model by giving it the correct answer.\n",
    "def create_generation_prompt(question, options, correct_answer_letter):\n",
    "    options_str = \"\\n\".join([f\"{key}: {value}\" for key, value in options.items()])\n",
    "    prompt = (\n",
    "        f\"You are an expert in pharmaceutical therapeutics. Your task is to answer the following multiple-choice question \"\n",
    "        f\"and provide a step-by-step reasoning process. The correct final answer is known to be '{correct_answer_letter}'. \"\n",
    "        f\"Explain in detail why '{correct_answer_letter}' is correct and the other options are wrong.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\nOptions:\\n{options_str}\\n\\n\"\n",
    "        f\"The correct answer is {correct_answer_letter}.\"\n",
    "    )\n",
    "    \n",
    "    # Use the same ChatML format we used for training\n",
    "    chat_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    return chat_prompt\n",
    "\n",
    "# --- Generate Reasoning ---\n",
    "results = []\n",
    "for _, row in tqdm(incorrect_df.iterrows(), total=len(incorrect_df), desc=\"Generating Reasoning\"):\n",
    "    # Create the specific prompt for this question\n",
    "    prompt = create_generation_prompt(row['question'], row['options'], row['correct_answer'])\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate the output\n",
    "    # We use do_sample=True and a non-zero temperature to encourage more creative/natural reasoning\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and clean up the response\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    # The response will be everything after the final 'assistant\\n'\n",
    "    reasoning_and_answer = generated_text.split(\"<|im_start|>assistant\\n\")[-1]\n",
    "    # Remove the final end token if it exists\n",
    "    reasoning_and_answer = reasoning_and_answer.replace(\"<|im_end|>\", \"\").strip()\n",
    "    \n",
    "    results.append({\n",
    "        \"id\": row['id'],\n",
    "        \"question\": row['question'],\n",
    "        \"options\": row['options'],\n",
    "        \"correct_answer_letter\": row['correct_answer'],\n",
    "        \"chosen_response\": reasoning_and_answer # This is the high-quality response for DPO\n",
    "    })\n",
    "\n",
    "# --- Save the \"Chosen\" Dataset for DPO ---\n",
    "chosen_df = pd.DataFrame(results)\n",
    "chosen_df.to_json(\"dpo_chosen_responses.jsonl\", orient=\"records\", lines=True)\n",
    "print(\"Successfully generated and saved the 'chosen' responses for DPO.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
