{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed  # Use ThreadPool instead of ProcessPool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_with_mps():\n",
    "    # Enable MPS and configure for Apple Silicon\n",
    "    llm = Llama.from_pretrained(\n",
    "        repo_id=\"mradermacher/TxAgent-T1-Llama-3.1-8B-GGUF\",\n",
    "        filename=\"TxAgent-T1-Llama-3.1-8B.Q8_0.gguf\",\n",
    "        # n_ctx=2048,\n",
    "        n_gpu_layers=16,  # Offload all possible layers to MPS (auto-detect)\n",
    "        n_threads=2,      # Use CPU threads for remaining work\n",
    "        n_batch=32,      # Larger batch size for MPS efficiency\n",
    "        verbose=False,     # Show MPS initialization logs\n",
    "        device=\"cpu\"      # Explicitly use Metal Performance Shaders\n",
    "    )\n",
    "    \n",
    "    # Verify MPS usage (check logs for confirmation)\n",
    "    # Look for lines like: \"llama_metal_init: loaded kernel\" or \"offloaded X layers to GPU\"\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = load_model_with_mps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk, llm):\n",
    "    \"\"\"Process a single chunk of samples\"\"\"\n",
    "    chunk_results = []\n",
    "    for sample in chunk:\n",
    "        try:\n",
    "            prompt = f\"\"\"Please answer the question: {sample['question']}. These are the options {sample['options']}.\"\"\"\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a reasoning clinical assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            \n",
    "            # Small delay to prevent MPS overload\n",
    "            time.sleep(0.3)\n",
    "            \n",
    "            output = llm.create_chat_completion(\n",
    "                messages=messages,\n",
    "                # max_tokens=64,\n",
    "                # temperature=0.0,\n",
    "                stream=False\n",
    "            )\n",
    "            \n",
    "            chunk_results.append({\n",
    "                'id': sample['id'],\n",
    "                'llm_answer': output['choices'][0]['message']['content']\n",
    "            })\n",
    "            print(f\"Processed sample {sample['id']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in sample {sample['id']}: {str(e)}\")\n",
    "            chunk_results.append({'id': sample['id'], 'error': str(e)})\n",
    "    \n",
    "    return chunk_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(data, num_chunks):\n",
    "    \"\"\"Split data into N equal chunks\"\"\"\n",
    "    chunk_size = len(data) // num_chunks\n",
    "    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "    # Handle any remaining samples in the last chunk\n",
    "    if len(chunks) > num_chunks:\n",
    "        chunks[num_chunks-1].extend(chunks[num_chunks:])\n",
    "        chunks = chunks[:num_chunks]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample, llm):\n",
    "    \"\"\"Process a single sample with the loaded model\"\"\"\n",
    "    prompt = f\"\"\"Please answer the question: {sample['question']}. These are the options {sample['options']}.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a reasoning clinical assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Generate answer with reduced max_tokens if possible (faster)\n",
    "    output = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        # max_tokens=512,  # 512 may be excessive for Q&A; test what works\n",
    "        # temperature=0.0  # Deterministic output (faster than higher temps)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'id': sample['id'],\n",
    "        'question_type': sample['question_type'],\n",
    "        'question': sample['question'],\n",
    "        'options': sample['options'],\n",
    "        'llm_answer': output['choices'][0]['message']['content'],\n",
    "        'llm_output': str(output)\n",
    "    }\n",
    "\n",
    "def main(data, output_json_path, num_chunks=2):  # num_chunks = desired parallelism\n",
    "    # Load existing results\n",
    "    if os.path.exists(output_json_path):\n",
    "        with open(output_json_path, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        results = []\n",
    "    \n",
    "    # Filter out already processed samples\n",
    "    processed_ids = {r['id'] for r in results}\n",
    "    to_process = [s for s in data if s['id'] not in processed_ids]\n",
    "    print(f\"Need to process {len(to_process)} samples split into {num_chunks} chunks\")\n",
    "\n",
    "    # Split data into chunks for parallel processing\n",
    "    chunks = split_into_chunks(to_process, num_chunks)\n",
    "\n",
    "    # Load model once (shared across threads)\n",
    "    print(\"Loading model...\")\n",
    "    llm = load_model_with_mps()\n",
    "    time.sleep(2)  # Let MPS initialize\n",
    "\n",
    "    # Process chunks in parallel\n",
    "    with ThreadPoolExecutor(max_workers=num_chunks) as executor:\n",
    "        # Map each chunk to a processing task\n",
    "        futures = [executor.submit(process_chunk, chunk, llm) for chunk in chunks]\n",
    "        \n",
    "        # Collect results from all chunks\n",
    "        for future in futures:\n",
    "            chunk_results = future.result()\n",
    "            results.extend(chunk_results)\n",
    "            \n",
    "            # Save progress after each chunk is done\n",
    "            with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "            print(f\"Completed a chunk. Total processed: {len(results)}\")\n",
    "\n",
    "    # Final save\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"All done! Total results: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your test data here (replace with actual data loading)\n",
    "with open('data/curebench_testset_phase1_mini.jsonl', 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "output_path = \"data/curebench_results_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = process_sample(data[1], llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to process 9 samples split into 2 chunks\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in sample QjaQymRAabrS: llama_decode returned -1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# main(data, output_path, max_workers=2) # leads to overkill\n",
    "main(data, output_path, num_chunks=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
