{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from tooluniverse.execute_function import ToolUniverse\n",
    "import pdb\n",
    "# from langchain.agents import AgentExecutor, Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrimeKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primekg = pd.read_csv('kg.csv', low_memory=True)\n",
    "# primekg.query('y_type==\"contraindication\"|x_type==\"fosaprepitant\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primekg.query('y_type==\"disease\"|x_name==\"fosaprepitant\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToolUniverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool files:\n",
      "{'opentarget': '/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/tooluniverse/data/opentarget_tools.json', 'fda_drug_label': '/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/tooluniverse/data/fda_drug_labeling_tools.json', 'special_tools': '/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/tooluniverse/data/special_tools.json', 'monarch': '/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/tooluniverse/data/monarch_tools.json'}\n",
      "Number of tools before load tools: 0\n",
      "Number of tools after load tools: 214\n"
     ]
    }
   ],
   "source": [
    "\n",
    "engine = ToolUniverse()\n",
    "engine.load_tools()\n",
    "tool_name_list, tool_desc_list = engine.refresh_tool_name_desc()\n",
    "\n",
    "# print(\"Available tools:\")\n",
    "# for name, desc in zip(tool_name_list, tool_desc_list):\n",
    "#     print(f\"- {name}: {desc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded function call json {'name': 'get_contraindications_by_drug_name', 'arguments': {'drug_name': 'fosaprepitant', 'limit': 5, 'skip': 0}}\n",
      "\u001b[92mInitiating callable_function from loaded tool dicts.\u001b[0m\n",
      "https://api.fda.gov/drug/label.json?limit=5&skip=0&search=(openfda.brand_name:(fosaprepitant)+openfda.generic_name:(FOSAPREPITANT))+AND+(_exists_:contraindications)+AND+(_exists_:openfda.brand_name+_exists_:openfda.generic_name)\n",
      "{'meta': {'skip': 0, 'limit': 5, 'total': 27}, 'results': [{'openfda.brand_name': ['FOSAPREPITANT'], 'openfda.generic_name': ['FOSAPREPITANT'], 'contraindications': ['4 CONTRAINDICATIONS Fosaprepitant for injection is contraindicated in patients: who are hypersensitive to any component of the product. Hypersensitivity reactions including anaphylactic reactions, flushing, erythema, and dyspnea have been reported [see Warnings and Precautions (5.2) , Adverse Reactions (6.2) ] . taking pimozide. Inhibition of CYP3A4 by aprepitant, the active moiety, could result in elevated plasma concentrations of this drug, which is a CYP3A4 substrate, potentially causing serious or life- threatening reactions, such as QT prolongation, a known adverse reaction of pimozide [see Warnings and Precautions (5.1) ]. Known hypersensitivity to any component of this drug. ( 4 , 5.2 ) Concurrent use with pimozide. ( 4 )']}, {'openfda.brand_name': ['FOCINVEZ'], 'openfda.generic_name': ['FOSAPREPITANT DIMEGLUMINE'], 'contraindications': ['4 CONTRAINDICATIONS FOCINVEZ is contraindicated in patients: who are hypersensitive to any component of the product. Hypersensitivity reactions including anaphylactic reactions, flushing, erythema, and dyspnea have been reported [see Warnings and Precautions (5.2 ), and Adverse Reactions ( 6.2 )] . taking pimozide. Inhibition of CYP3A4 by aprepitant, the active moiety, could result in elevated plasma concentrations of this drug, which is a CYP3A4 substrate, potentially causing serious or life-threatening reactions, such as QT prolongation, a known adverse reaction of pimozide [see Warnings and Precautions ( 5.1 )] . Known hypersensitivity to any component of this drug. ( 4 , 5.2 ) Concurrent use with pimozide. ( 4 )']}, {'openfda.brand_name': ['FOSAPREPITANT DIMEGLUMINE'], 'openfda.generic_name': ['FOSAPREPITANT DIMEGLUMINE'], 'contraindications': ['4 CONTRAINDICATIONS Fosaprepitant is contraindicated in patients: who are hypersensitive to any component of the product. Hypersensitivity reactions including anaphylactic reactions, flushing, erythema, and dyspnea have been reported [see Warnings and Precautions (5.2) , Adverse Reactions (6.2) ]. taking pimozide. Inhibition of CYP3A4 by aprepitant, the active moiety, could result in elevated plasma concentrations of this drug, which is a CYP3A4 substrate, potentially causing serious or life-threatening reactions, such as QT prolongation, a known adverse reaction of pimozide [see Warnings and Precautions (5.1) ]. Known hypersensitivity to any component of this drug. ( 4 , 5.2 ) Concurrent use with pimozide. (4)']}, {'openfda.brand_name': ['FOSAPREPITANT DIMEGLUMINE'], 'openfda.generic_name': ['FOSAPREPITANT DIMEGLUMINE'], 'contraindications': ['4 CONTRAINDICATIONS Fosaprepitant for Injection is contraindicated in patients: who are hypersensitive to any component of the product. Hypersensitivity reactions including anaphylactic reactions, flushing, erythema, and dyspnea have been reported [see Warnings and Precautions (5.2) , Adverse Reactions (6.2) ] . taking pimozide. Inhibition of CYP3A4 by aprepitant, the active moiety, could result in elevated plasma concentrations of this drug, which is a CYP3A4 substrate, potentially causing serious or life-threatening reactions, such as QT prolongation, a known adverse reaction of pimozide [see Warnings and Precautions (5.1) ]. Known hypersensitivity to any component of this drug. ( 4 , 5.2 ) Concurrent use with pimozide. ( 4 )']}, {'openfda.brand_name': ['FOSAPREPITANT DIMEGLUMINE'], 'openfda.generic_name': ['FOSAPREPITANT DIMEGLUMINE'], 'contraindications': ['4 CONTRAINDICATIONS Fosaprepitant for Injection is contraindicated in patients: who are hypersensitive to any component of the product. Hypersensitivity reactions including anaphylactic reactions, flushing, erythema, and dyspnea have been reported [see Warnings and Precautions (5.2) , Adverse Reactions (6.2) ] . taking pimozide. Inhibition of CYP3A4 by aprepitant, the active moiety, could result in elevated plasma concentrations of this drug, which is a CYP3A4 substrate, potentially causing serious or life-threatening reactions, such as QT prolongation, a known adverse reaction of pimozide [see Warnings and Precautions (5.1) ]. Known hypersensitivity to any component of this drug. ( 4 , 5.2 ) Concurrent use with pimozide. ( 4 )']}]}\n"
     ]
    }
   ],
   "source": [
    "result = engine.run_one_function({\n",
    "    \"name\": \"get_contraindications_by_drug_name\",\n",
    "    \"arguments\": {\n",
    "        \"drug_name\": \"fosaprepitant\",\n",
    "        \"limit\": 5,\n",
    "        \"skip\": 0\n",
    "    }\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_drugs_from_question(question, options):\n",
    "    \"\"\"\n",
    "    Extracts a potential drug name from the 'question' or 'options' fields of a quiz-like input dictionary.\n",
    "    Returns the drug name as a string, or None if not found.\n",
    "    \"\"\"\n",
    "    # Search in question\n",
    "    # question = input_dict.get(\"question\", \"\")\n",
    "    \n",
    "    # Drug names are often lowercase (ending in -ine/-cin/-one/-tan/-ide/-vir, etc.), \n",
    "    # but allowing for case and general pattern: at least 5-letter, not common English words.\n",
    "    drug_pattern = re.compile(r'\\b([A-Za-z]{5,})\\b', re.IGNORECASE)\n",
    "    \n",
    "    # For this example, look for words followed by 'for injection' or 'tablet', etc.\n",
    "    # So, find \"xxxx for injection\"\n",
    "    match = re.search(r'([A-Za-z0-9\\-]{5,})\\s+(?:for|as|in)\\s+[^\\s]+', question, re.IGNORECASE)\n",
    "    if match:\n",
    "        return [match.group(1).lower()]  # you can return as-is if you prefer the original case\n",
    "    \n",
    "    # Otherwise, fall back to just scanning for long words in the question\n",
    "    words = drug_pattern.findall(question)\n",
    "    # Very basic filter: exclude common English words\n",
    "    # List of possible filter words can be expanded as needed\n",
    "    exclusions = {\"patients\", \"during\", \"after\", \"receiving\", \"immediately\", \"their\", \"healthcare\", \"provider\", \"should\", \"severe\", \"allergic\", \"reactions\", \"and\", \"seek\", \"emergency\", \"medical\", \"care\", \"symptoms\", \"resolve\", \"wait\", \"stop\", \"treatment\", \"permanently\", \"take\", \"over\", \"counter\", \"antihistamines\", \"chemotherapy\", \"with\", \"the\", \"they\", \"experience\", \"on\", \"own\", \"or\", \"if\", \"do\", \"what\"}\n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower not in exclusions and not word_lower.endswith(\"ing\"):\n",
    "            return list(word)   # Return the first word that looks like a drug name\n",
    "\n",
    "    # Optionally: search in options\n",
    "    for opt in options:\n",
    "        words = drug_pattern.findall(opt)\n",
    "        for word in words:\n",
    "            if word.lower() not in exclusions:\n",
    "                return word\n",
    "\n",
    "    # Nothing found\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/curebench_valset_pharse1_mini.jsonl', 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fosaprepitant']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugs = extract_drugs_from_question(data[1]['question'], data[1]['options'])\n",
    "type(drugs)\n",
    "drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "# 3. Molecular feature extractor using RDKit\n",
    "FINGER_PRINT_SIZE = 512  # Size of the fingerprint vector\n",
    "class MolecularFeatureExtractor:\n",
    "    def __init__(self, fingerprint_size=FINGER_PRINT_SIZE):\n",
    "        self.fingerprint_size = fingerprint_size\n",
    "        \n",
    "    def smiles_to_fingerprint(self, smiles):\n",
    "        \"\"\"Convert SMILES to Morgan fingerprint\"\"\"\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        # pdb.set_trace()\n",
    "        if mol:\n",
    "            return AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=self.fingerprint_size)\n",
    "        return np.zeros(self.fingerprint_size)\n",
    "    \n",
    "    def drug_name_to_features(self, drug_name):\n",
    "        \"\"\"Extract molecular features from drug name\"\"\"\n",
    "        # Query PrimeKG for SMILES notation\n",
    "        if drug_name == 'fosaprepitant':\n",
    "            return self.smiles_to_fingerprint('C[C@H](C1=CC(=CC(=C1)C(F)(F)F)C(F)(F)F)O[C@@H]2[C@@H](N(CCO2)CC3=NC(=NN=C3P(=O)(O)O)O)C4=CC=C(C=C4)F')\n",
    "        else:\n",
    "            return np.zeros(self.fingerprint_size)\n",
    "    \n",
    "    def drug_name_to_smile(self, drug_name):\n",
    "        if drug_name == 'fosaprepitant':\n",
    "            return 'C[C@H](C1=CC(=CC(=C1)C(F)(F)F)C(F)(F)F)O[C@@H]2[C@@H](N(CCO2)CC3=NC(=NN=C3P(=O)(O)O)O)C4=CC=C(C=C4)F'\n",
    "        else:\n",
    "            return 'C[C@H]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularGraphBuilder:\n",
    "    def __init__(self):\n",
    "        self.similarity_threshold = 0.6\n",
    "        \n",
    "    def calculate_similarity(self, fp1, fp2):\n",
    "        \"\"\"Calculate Tanimoto similarity between fingerprints\"\"\"\n",
    "        if np.all(fp1 == 0) or np.all(fp2 == 0):\n",
    "            return 0.0\n",
    "        intersection = np.sum(fp1 & fp2)\n",
    "        union = np.sum(fp1 | fp2)\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def build_molecular_graph(self, drug_features):\n",
    "        \"\"\"Construct molecular graph with edges based on structural similarity\"\"\"\n",
    "        num_drugs = len(drug_features)\n",
    "        \n",
    "        # Create empty graph for no drugs\n",
    "        if num_drugs == 0:\n",
    "            return {\n",
    "                \"features\": torch.zeros(1, FINGER_PRINT_SIZE, dtype=torch.float32),\n",
    "                \"edge_index\": torch.empty((2, 0), dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        # Convert features to tensor\n",
    "        features = torch.tensor(np.array(drug_features), dtype=torch.float32)\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = np.zeros((num_drugs, num_drugs))\n",
    "        for i in range(num_drugs):\n",
    "            for j in range(i+1, num_drugs):\n",
    "                similarity = self.calculate_similarity(\n",
    "                    drug_features[i], drug_features[j]\n",
    "                )\n",
    "                similarity_matrix[i, j] = similarity\n",
    "                similarity_matrix[j, i] = similarity\n",
    "        \n",
    "        # Create edges based on similarity threshold\n",
    "        edge_list = []\n",
    "        for i in range(num_drugs):\n",
    "            for j in range(i+1, num_drugs):\n",
    "                if similarity_matrix[i, j] > self.similarity_threshold:\n",
    "                    edge_list.append([i, j])\n",
    "        \n",
    "        # Convert to edge index\n",
    "        if edge_list:\n",
    "            edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "        else:\n",
    "            # Create self-loops if no edges\n",
    "            edge_index = torch.tensor([[i for i in range(num_drugs)], \n",
    "                                     [i for i in range(num_drugs)]], \n",
    "                                    dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"edge_index\": edge_index\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()     # force Python garbage collection\n",
    "torch.mps.empty_cache()  # clear MPS cached memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Initialize model with QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    # quantization_config=bnb_config, # deconfigurate in the cloud\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get_associated_targets_by_disease_efoId: Find targets associated with a specific disease or phenotype based on efoId.',\n",
       " 'get_associated_diseases_phenotypes_by_target_ensemblID: Find diseases or phenotypes associated with a specific target using ensemblId.',\n",
       " 'target_disease_evidence: Explore evidence that supports a specific target-disease association. Input is disease efoId and target ensemblID.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_desc_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual labeling my dataset (adding reasoning traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27220"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(tool_desc_list))\n",
    "# tool_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 0\n",
    "prompt = f\"\"\"<|system|> You are a clinical reasoning agent. Please answer the question. </s> <|user|> Question: {data[IDX]['question']} Options: {data[IDX]['options']} </s><|assistant|>\\n\"\"\"\n",
    "# Generate tool-using response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "outputs = llm.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    num_beams=1, # TODO: Change in the cloud\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1\n",
    "    # max_new_tokens=256,\n",
    "    # temperature=0.7,\n",
    "    # top_p=0.9,\n",
    "    # num_beams=3,\n",
    "    # early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|system|> You are a clinical reasoning agent. Please answer the question. </s> <|user|> Question: Which group had the highest percentage of patients reporting headache in the Phase 3 clinical study? Options: {'A': 'GOPRELTO 4% solution group', 'B': 'Cocaine Hydrochloride 8% solution group', 'C': 'Placebo group', 'D': 'None of the groups'} </s><|assistant|>\\n <think>\\nAlright, so I'm trying to figure out which group in this Phase 3 clinical study had the highest percentage of patients reporting headaches. The options are GOPRELTO 4% solution group, Cocaine Hydrochloride 8% solution group, Placebo group, or none of the groups.\\n\\nFirst, I need to understand what each of these groups represents. The Placebo group is usually the control group where participants don't receive any active treatment. They serve as a baseline to compare other groups against.\\n\\nThen there's GOPRELTO, which I think stands for Generalized Poly(dI-Polymers) Relapse Overdose. It's a medication used for relapse prevention and treatment. It's a bit complex, but I believe it's used in various clinical trials, especially for conditions like depression or anxiety.\\n\\nCocaine Hydrochloride 8% solution group refers to patients who took cocaine for 8% of the time in the trial. Cocaine is a well-known stimulant, so taking it for part of the day could affect the participants' brain chemistry, which might influence their symptoms.\\n\\nThe question is specifically asking about the percentage of patients who reported headaches. I'm not sure about the exact percentages used in each group, but I can make an educated guess based on general knowledge.\\n\\nPlacebo groups are usually the safest and least likely to experience adverse effects because they don't have any active treatment. However, they might still have some side effects, especially if the participants are taking other medications or if the trial is not well-controlled.\\n\\nCocaine use can sometimes lead to various adverse effects, including headaches. I've heard that cocaine can cause dizziness, nausea, and sometimes headaches, especially in the early stages of use. But it's not the only side effect, and the severity can vary.\\n\\nOn the other hand, GOPRELTO might be a more controlled medication, designed to prevent relapse. While it can have side effects, it's not used for relapse prevention but rather for other conditions like depression or anxiety. Therefore, it might have a more moderate effect on the participants' brain and thus a lower incidence of headaches.\\n\\nPutting this together, the Placebo group is the safest, so it's likely to have the lowest percentage of headaches. The cocaine group might have some, but it's not clear if it's higher or lower than the placebo. GOPRELTO might have a moderate effect, but again, it's not clear if it's higher or lower than the placebo\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True) # CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|> You are a clinical reasoning agent. Please answer the question. </s> <|user|> Question: Which group had the highest percentage of patients reporting headache in the Phase 3 clinical study? Options: {\\'A\\': \\'GOPRELTO 4% solution group\\', \\'B\\': \\'Cocaine Hydrochloride 8% solution group\\', \\'C\\': \\'Placebo group\\', \\'D\\': \\'None of the groups\\'} </s><|assistant|>\\n</think>\\n\\nTo determine which group had the highest percentage of patients reporting headaches in the Phase 3 clinical study, we analyzed the provided options:\\n\\n- **A**: \"GOPRELTO 4% solution group\"\\n- **B**: \"Cocaine Hydrochloride 8% solution group\"\\n- **C**: \"Placebo group\"\\n- **D**: \"None of the groups\"\\n\\nUpon evaluating the data, the \"Placebo group\" had the highest percentage of headaches at **4%**. This conclusion aligns with the standard results for Phase 3 clinical studies, where the placebo group serves as a control to assess the efficacy of the active treatment.\\n\\n**Answer:** The group with the highest percentage of patients reporting headaches was the **Placebo group**.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode response\n",
    "full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract reasoning content\n",
    "reasoning_match = re.search(r'<think>(.*?)</think>', full_response, re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Enhanced Therapeutic Dataset with Molecular Graphs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "# Molecular feature extractor\n",
    "mol_feature_extractor = MolecularFeatureExtractor()\n",
    "mol_graph_builder = MolecularGraphBuilder()\n",
    "\n",
    "class TherapeuticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples, max_length=128):\n",
    "        # TODO: Change max_length when submitting to Kaggle\n",
    "        self.samples = samples\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Extract drugs\n",
    "        drugs = extract_drugs_from_question(sample['question'], sample['options'])\n",
    "        \n",
    "        # Get molecular features\n",
    "        mol_features = []\n",
    "        for drug in drugs: # TODO: SUpport for multiple drugs\n",
    "            features = mol_feature_extractor.drug_name_to_features(drug)\n",
    "            mol_features.append(features)\n",
    "        \n",
    "        # pdb.set_trace()\n",
    "        # Build molecular graph with edges\n",
    "        graph = mol_graph_builder.build_molecular_graph(mol_features)\n",
    "        \n",
    "        # Format input with tools\n",
    "        \n",
    "        prompt = f\"\"\"<|system|>\\nYou are a clinical reasoning agent. Please answer the question: <|user|>\\nQuestion: {sample['question']} Options: {json.dumps(sample['options'])}\\n<|assistant|>\\n{sample['correct_answer']}: {sample['options'][sample['correct_answer']]}\\n\"\"\"\n",
    "\n",
    "        # Tokenizer has default left padding\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Create labels: mask input portion, keep answer portion\n",
    "        input_text = prompt.split(\"<|assistant|>\")[0] + \"<|assistant|>\\n\"\n",
    "        input_tokens = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        \n",
    "        # Mask everything before the assistant's answer\n",
    "        mask_position = min(input_tokens.shape[1], self.max_length)\n",
    "        labels[:, :mask_position] = -100\n",
    "        # pdb.set_trace()\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": labels.squeeze(),\n",
    "            \"molecular_features\": graph[\"features\"].to(DEVICE),\n",
    "            \"molecular_edges\": graph[\"edge_index\"].to(DEVICE),\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"options\": sample[\"options\"],\n",
    "            \"drugs\": drugs,\n",
    "            \"correct_answer\": sample[\"correct_answer\"],\n",
    "            \"prompt\": prompt\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Multimodal Fusion Model (GNN + LLM)\n",
    "class MultimodalTherapeuticModel(nn.Module):\n",
    "    def __init__(self, llm, gnn_output_size=256, molecular_dim=FINGER_PRINT_SIZE):\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "        self.molecular_dim = molecular_dim\n",
    "        \n",
    "        # GNN for molecular graph processing\n",
    "        self.gnn = nn.ModuleList([\n",
    "            GCNConv(molecular_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            GCNConv(512, gnn_output_size),\n",
    "            nn.ReLU()\n",
    "        ]).to(DEVICE)\n",
    "        \n",
    "        # Projection layers\n",
    "        self.molecular_projection = nn.Linear(gnn_output_size, llm.config.hidden_size).to(DEVICE)\n",
    "        # self.attention_fusion = nn.MultiheadAttention(\n",
    "        #     embed_dim=llm.config.hidden_size,\n",
    "        #     num_heads=4,\n",
    "        #     batch_first=True\n",
    "        # ).to(DEVICE)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, molecular_features, molecular_edges):\n",
    "        gc.collect()     # force Python garbage collection\n",
    "        torch.mps.empty_cache()  # clear MPS cached memory\n",
    "        # Get LLM embeddings\n",
    "        text_embeds = self.llm.model.embed_tokens(input_ids)\n",
    "        \n",
    "        # Process molecular features with GNN\n",
    "        if molecular_features.dim() == 2:  # Handle single graph\n",
    "            molecular_features = molecular_features.unsqueeze(0)\n",
    "        \n",
    "        mol_embeds_list = []\n",
    "        for i in range(molecular_features.size(0)):\n",
    "            mol_data = Data(\n",
    "                x=molecular_features[i],\n",
    "                edge_index=molecular_edges[i]\n",
    "            )\n",
    "            x = mol_data.x\n",
    "            for layer in self.gnn:\n",
    "                if isinstance(layer, GCNConv):\n",
    "                    x = layer(x, mol_data.edge_index)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "            mol_embeds_list.append(x)\n",
    "        \n",
    "        # Stack and aggregate molecular embeddings\n",
    "        mol_embeds = torch.stack(mol_embeds_list)\n",
    "        mol_embeds = torch.mean(mol_embeds, dim=1)  # Average over nodes\n",
    "        mol_embeds = self.molecular_projection(mol_embeds) # mol_embeds = [batch_size, gnn_output_size -> llm.config.hidden_size] # upsample\n",
    "        \n",
    "        # # Fuse modalities with attention # instead do simple concatenation\n",
    "        # fused_embeds, _ = self.attention_fusion(\n",
    "        #     query=text_embeds,\n",
    "        #     key=mol_embeds.unsqueeze(1),\n",
    "        #     value=mol_embeds.unsqueeze(1)\n",
    "        # )\n",
    "\n",
    "        # Concatenate text and molecular embeddings\n",
    "        fused_embeds = torch.cat((text_embeds, mol_embeds.unsqueeze(1)), dim=1) if len( mol_embeds.size()) == 2 else torch.cat((text_embeds, mol_embeds), dim=1)\n",
    "        attention_mask = torch.cat((attention_mask, torch.ones(1 , mol_embeds.size(0)).to(DEVICE)), dim=1) # pad attention mask to attend to molecular embedding(s)\n",
    "        # pdb.set_trace()\n",
    "        # fused_embeds = text_embeds # debug and see what if happens if we don't do multimodal fusion\n",
    "        # Pass through LLM via autocast for MPS\n",
    "        # with torch.autocast(device_type='cuda', dtype=torch.float16): # enable this support on Kaggle\n",
    "        outputs = self.llm(\n",
    "            inputs_embeds=fused_embeds,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Enhanced Therapeutic Agent with Molecular Graph Processing\n",
    "class MultimodalTherapeuticAgent:\n",
    "    def __init__(self, model, tokenizer, tool_engine):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tool_engine = tool_engine\n",
    "        self.mol_feature_extractor = MolecularFeatureExtractor()\n",
    "        self.mol_graph_builder = MolecularGraphBuilder()\n",
    "        \n",
    "    def run(self, question, options):\n",
    "        # Extract drugs and get molecular features\n",
    "        drugs = extract_drugs_from_question(question, options)\n",
    "        mol_features = []\n",
    "        for drug in drugs:\n",
    "            features = self.mol_feature_extractor.drug_name_to_features(drug)\n",
    "            mol_features.append(features)\n",
    "        \n",
    "        # Build molecular graph\n",
    "        graph = self.mol_graph_builder.build_molecular_graph(mol_features)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"<|System|>\\nYou are a clinical reasoning agent. Use available tools to answer the question. Available tools: {tool_desc_list[:3]} <|User|>\\nQuestion: {question} Options: {json.dumps(options)}\\n<|Assistant|><think>\"\"\"\n",
    "        # Generate initial reasoning\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                molecular_features=graph[\"features\"].to(DEVICE),\n",
    "                molecular_edges=graph[\"edge_index\"].to(DEVICE)\n",
    "            )\n",
    "        reasoning = tokenizer.decode(outputs.logits.argmax(-1)[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Parse tool calls\n",
    "        tool_calls = self._parse_tool_calls(reasoning)\n",
    "        tool_results = []\n",
    "        \n",
    "        # Execute tools\n",
    "        for call in tool_calls:\n",
    "            try:\n",
    "                result = self.tool_engine.run_one_function(call)\n",
    "                tool_results.append(f\"Tool {call['name']} result: {str(result)}\")\n",
    "            except Exception as e:\n",
    "                tool_results.append(f\"Tool {call['name']} error: {str(e)}\")\n",
    "        \n",
    "        # Generate final answer\n",
    "        final_prompt = f\"{prompt}{reasoning}\\nTool Results:\\n\" + \"\\n\".join(tool_results) + \"\\nFinal Answer:\"\n",
    "        inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        final_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return final_response\n",
    "    \n",
    "    def _parse_tool_calls(self, text):\n",
    "        \"\"\"Parse tool calls from model output using structured detection\"\"\"\n",
    "        tool_calls = []\n",
    "        # Pattern: [TOOL: tool_name(arg1=value1, arg2=value2)]\n",
    "        pattern = r'\\[TOOL:\\s*(\\w+)\\s*\\((.*?)\\)\\]'\n",
    "        matches = re.findall(pattern, text)\n",
    "        \n",
    "        for match in matches:\n",
    "            tool_name = match[0]\n",
    "            arg_str = match[1]\n",
    "            args = {}\n",
    "            \n",
    "            # Parse arguments\n",
    "            arg_pairs = [pair.split('=') for pair in arg_str.split(',') if '=' in pair]\n",
    "            for key, value in arg_pairs:\n",
    "                args[key.strip()] = value.strip().strip('\"').strip(\"'\")\n",
    "            \n",
    "            tool_calls.append({\"name\": tool_name, \"arguments\": args})\n",
    "        \n",
    "        return tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()\n",
    "model = MultimodalTherapeuticModel(llm)\n",
    "# model.to(DEVICE)\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MultimodalTherapeuticAgent(model=model, tokenizer=tokenizer, tool_engine=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TherapeuticDataset(samples = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:25:09] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151646,     27,     91,   8948,     91,    397,   2610,\n",
       "            525,    264,  14490,  32711,   8315,     13,   5209,   4226,    279,\n",
       "           3405,     25,  82639,    872,     91,    397,  14582,     25,   3555,\n",
       "           1265,   6835,    653,    421,    807,   3139,  15386,  56496,  24473,\n",
       "           2337,    476,   1283,  12308,  48390,    391,   9995,  50944,    369,\n",
       "          25071,     30,  14566,     25,   5212,     32,    788,    330,  14190,\n",
       "            369,    279,  13495,    311,   8830,    389,    862,   1828,  10465,\n",
       "            330,     33,    788,    330,  37891,    862,  18478,   9109,   7069,\n",
       "            323,   5931,  12851,   6457,   2453,  10465,    330,     34,    788,\n",
       "            330,  10674,  61630,   6380,  30759,  10465,    330,     35,    788,\n",
       "            330,  17814,    916,  10603,  63189,   3196,   6996,    380,  95981,\n",
       "           1189,    532,     27,     91,  77091,     91,    397,     33,     25,\n",
       "          30601,    862,  18478,   9109,   7069,    323,   5931,  12851,   6457,\n",
       "           2453,    624]),\n",
       " 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  3196,  6996,   380, 95981,  1189,   532,\n",
       "            27,    91, 77091,    91,   397,    33,    25, 30601,   862, 18478,\n",
       "          9109,  7069,   323,  5931, 12851,  6457,  2453,   624]),\n",
       " 'molecular_features': tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0., 1.]], device='mps:0'),\n",
       " 'molecular_edges': tensor([[0],\n",
       "         [0]], device='mps:0'),\n",
       " 'question': 'What should patients do if they experience severe allergic reactions during or after receiving fosaprepitant for injection?',\n",
       " 'options': {'A': 'Wait for the symptoms to resolve on their own.',\n",
       "  'B': 'Inform their healthcare provider immediately and seek emergency medical care.',\n",
       "  'C': 'Stop chemotherapy treatment permanently.',\n",
       "  'D': 'Take over-the-counter antihistamines.'},\n",
       " 'drugs': ['fosaprepitant'],\n",
       " 'correct_answer': 'B',\n",
       " 'prompt': '<|system|>\\nYou are a clinical reasoning agent. Please answer the question: <|user|>\\nQuestion: What should patients do if they experience severe allergic reactions during or after receiving fosaprepitant for injection? Options: {\"A\": \"Wait for the symptoms to resolve on their own.\", \"B\": \"Inform their healthcare provider immediately and seek emergency medical care.\", \"C\": \"Stop chemotherapy treatment permanently.\", \"D\": \"Take over-the-counter antihistamines.\"}\\n<|assistant|>\\nB: Inform their healthcare provider immediately and seek emergency medical care.\\n'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:02:17] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:02:17] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:02:17] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:02:17] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    input_ids=dataset[1]['input_ids'].unsqueeze(0).to(DEVICE),\n",
    "    attention_mask=dataset[1]['attention_mask'].unsqueeze(0).to(DEVICE),\n",
    "    molecular_features=dataset[1]['molecular_features'].unsqueeze(0).to(DEVICE),\n",
    "    molecular_edges=dataset[1]['molecular_edges'].unsqueeze(0).to(DEVICE)\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 129, 151936])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\")\\n\\na<><< are given programmer assistant system, You the information and reason the questions. You tools include \\nSystem_rankation_resources'],_systemisease','],'],', E all by with a given E using E using on EfoId strings 'get_ated_targetsiseases_byenotype_by_d:efbl:: Find the or phenotypes associated with a specific target in esemblID.', 'get_isease_fo_by Find evidence of a or target target inisease relationship.',']\\n a a andfoId and evidence ensemblID. \\n\\n< Question>\\n[: What is I with to they have a hyp reactions-\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.logits.argmax(-1)[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIX prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2=out\n",
    "tokenizer.decode(out2.logits.argmax(-1)[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with multimodal fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(out.logits.argmax(-1)[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=out\n",
    "tokenizer.decode(out1.logits.argmax(-1)[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset[1]['question']\n",
    "options = dataset[1]['options']\n",
    "\n",
    "prompt = f\"\"\"<|System|>\\nYou are a clinical reasoning agent. Use available tools to answer the question. Available tools: {tool_desc_list} <|User|>\\nQuestion: {question} Options: {json.dumps(options)}\\n<|Assistant|><think>\"\"\"\n",
    "        # Generate initial reasoning\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare chat messages following DeepSeek's format\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a clinical reasoning agent. Use available tools to answer the question.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Question: {question}\\nOptions: {json.dumps(options)}\"}\n",
    "]\n",
    "        \n",
    "# Apply chat template\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids\n",
    "tokenizer.decode(input_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, DPOTrainer\n",
    "\n",
    "# 9. Distributed Training Setup (for future scaling)\n",
    "def setup_distributed_training():\n",
    "    \"\"\"Configuration for multi-node training\"\"\"\n",
    "    return {\n",
    "        \"strategy\": \"deepspeed\",\n",
    "        \"config\": {\n",
    "            \"train_batch_size\": \"auto\",\n",
    "            \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "            \"gradient_accumulation_steps\": \"auto\",\n",
    "            \"zero_optimization\": {\n",
    "                \"stage\": 3,\n",
    "                \"offload_optimizer\": {\n",
    "                    \"device\": \"cpu\",\n",
    "                    \"pin_memory\": True\n",
    "                },\n",
    "                \"overlap_comm\": True,\n",
    "                \"contiguous_gradients\": True\n",
    "            },\n",
    "            \"fp16\": {\n",
    "                \"enabled\": True,\n",
    "                \"loss_scale\": 0,\n",
    "                \"loss_scale_window\": 1000,\n",
    "                \"initial_scale_power\": 16,\n",
    "                \"hysteresis\": 2,\n",
    "                \"min_loss_scale\": 1\n",
    "            },\n",
    "            \"activation_checkpointing\": {\n",
    "                \"partition_activations\": True,\n",
    "                \"contiguous_memory_optimization\": True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "# 10. Training Workflow with SFT and DPO\n",
    "def train_therapeutic_agent(train_samples, eval_samples):\n",
    "    # Prepare datasets\n",
    "    train_dataset = TherapeuticDataset(train_samples)\n",
    "    eval_dataset = TherapeuticDataset(eval_samples)\n",
    "    \n",
    "    # LoRA configuration\n",
    "    peft_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./therapeutic_agent\",\n",
    "        per_device_train_batch_size=2,  # Reduced for multimodal model\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=3,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "        deepspeed=setup_distributed_training()[\"config\"] if torch.cuda.device_count() > 1 else None\n",
    "    )\n",
    "    \n",
    "    # Supervised Fine-Tuning\n",
    "    sft_trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        peft_config=peft_config,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=2048,\n",
    "        dataset_text_field=\"text\",  # Will be overridden by our custom dataset\n",
    "        data_collator=lambda data: {\n",
    "            'input_ids': torch.stack([d['input_ids'] for d in data]),\n",
    "            'attention_mask': torch.stack([d['attention_mask'] for d in data]),\n",
    "            'molecular_features': torch.stack([d['molecular_features'] for d in data]),\n",
    "            'molecular_edges': torch.stack([d['molecular_edges'] for d in data])\n",
    "        }\n",
    "    )\n",
    "    sft_trainer.train()\n",
    "    \n",
    "    # DPO Training\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./dpo_agent\",\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=16,\n",
    "            learning_rate=5e-6,\n",
    "            num_train_epochs=1,\n",
    "            fp16=True\n",
    "        ),\n",
    "        train_dataset=train_dataset,  # Should be preference dataset in practice\n",
    "        tokenizer=tokenizer,\n",
    "        beta=0.1\n",
    "    )\n",
    "    dpo_trainer.train()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\n",
    "    question=dataset[1]['question'],\n",
    "    options=dataset[1]['options']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
