{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12537244,"sourceType":"datasetVersion","datasetId":7914908},{"sourceId":12556983,"sourceType":"datasetVersion","datasetId":7928936},{"sourceId":159451933,"sourceType":"kernelVersion"},{"sourceId":220844327,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !nvcc --version && echo $CUDA_HOME\n# !nvidia-smi  # Should show a T4 or V100 GPU","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%bash\n# export FORCE_CMAKE=1\n# export CUDACXX=/usr/local/cuda/bin/nvcc\n# export CMAKE_ARGS=\"-DGGML_CUDA=ON -DCMAKE_CUDA_COMPILER=$CUDACXX\"\n!pip --quiet install --upgrade llama-cpp-python ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:03:20.424459Z","iopub.execute_input":"2025-07-23T00:03:20.424890Z","iopub.status.idle":"2025-07-23T00:03:24.643779Z","shell.execute_reply.started":"2025-07-23T00:03:20.424854Z","shell.execute_reply":"2025-07-23T00:03:24.642446Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# \nimport os\nimport json\nimport time\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\nfrom llama_cpp import Llama\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:08:49.809444Z","iopub.execute_input":"2025-07-23T00:08:49.809873Z","iopub.status.idle":"2025-07-23T00:08:49.947531Z","shell.execute_reply.started":"2025-07-23T00:08:49.809824Z","shell.execute_reply":"2025-07-23T00:08:49.946523Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def load_model():\n    # \"\"\"Load model with Kaggle CPU constraints\"\"\"\n    # os.environ[\"GGML_CUDA_FORCE_MMQ\"] = \"1\"\n    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Ensure Kaggle's GPU is visible\n    \n    llm = Llama.from_pretrained(\n        repo_id=\"mradermacher/TxAgent-T1-Llama-3.1-8B-GGUF\",\n        filename=\"TxAgent-T1-Llama-3.1-8B.Q8_0.gguf\",\n        # n_ctx=2048, # default 512\n        # n_gpu_layers=35,  # Kaggle GPUs (T4/V100) work best with ~35 layers (avoids OOM)\n        n_threads=2,  # Kaggle has 4 CPU cores - limit per-worker threads\n        # n_batch=128,  # Smaller batch to fit Kaggle GPU memory\n        verbose=False,  # Reduce log spam in Kaggle console\n        flash_attn=True,\n        # device='cuda'\n    )\n    # Verify GPU is detected\n    # if not llm.ctx.is_using_gpu:\n    #     print(\"GPU not detected! Check CUDA installation.\")\n    # print(\"Successfully using GPU acceleration!\")\n\n    \n    return llm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:08:52.269884Z","iopub.execute_input":"2025-07-23T00:08:52.270961Z","iopub.status.idle":"2025-07-23T00:08:52.277861Z","shell.execute_reply.started":"2025-07-23T00:08:52.270923Z","shell.execute_reply":"2025-07-23T00:08:52.276447Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def process_chunk(chunk, llm):\n    \"\"\"Process a single chunk of samples on Kaggle GPU\"\"\"\n    chunk_results = []\n    for sample in chunk:\n        try:\n            prompt = f\"\"\"Please answer the question: {sample['question']}. These are the options {sample['options']}.\"\"\"\n            \n            messages = [\n                {\"role\": \"system\", \"content\": \"You are a reasoning clinical assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n            \n            # Small delay to prevent GPU overload on Kaggle\n            time.sleep(0.2)\n            \n            output = llm.create_chat_completion(\n                messages=messages,\n                # max_tokens=64,\n                # temperature=0.0,1\n                stream=False\n            )\n            \n            chunk_results.append({\n                'id': sample['id'],\n                'llm_answer': output['choices'][0]['message']['content']\n            })\n            print(f\"Processed sample {sample['id']}\")\n            \n        except Exception as e:\n            print(f\"Error in sample {sample['id']}: {str(e)}\")\n            chunk_results.append({'id': sample['id'], 'error': str(e)})\n    \n    return chunk_results\n\ndef split_into_chunks(data, num_chunks):\n    \"\"\"Split data into equal chunks for parallel processing\"\"\"\n    chunk_size = len(data) // num_chunks\n    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n    # Handle remaining samples\n    if len(chunks) > num_chunks:\n        chunks[num_chunks-1].extend(chunks[num_chunks:])\n        chunks = chunks[:num_chunks]\n    return chunks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:08:54.330826Z","iopub.execute_input":"2025-07-23T00:08:54.331294Z","iopub.status.idle":"2025-07-23T00:08:54.341883Z","shell.execute_reply.started":"2025-07-23T00:08:54.331243Z","shell.execute_reply":"2025-07-23T00:08:54.340603Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def process_sample(sample):\n    \"\"\"Process single sample - model loaded per worker (Kaggle-safe)\"\"\"\n    llm = load_model()  # Load model once per worker (Kaggle memory-friendly)\n    \n    prompt = f\"\"\"Please answer the question: {sample['question']}. These are the options {sample['options']}.\"\"\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a reasoning clinical assistant.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    \n    output = llm.create_chat_completion(\n        messages=messages,\n        # max_tokens=128,  # Further reduced for Kaggle speed (clinical Q&A is concise)\n        # temperature=0.0,\n        stream=False,  # Disable streaming (faster in Kaggle)\n    )\n    \n    return {\n        'id': sample['id'],\n        'question_type': sample['question_type'],\n        'question': sample['question'],\n        'options': sample['options'],\n        'llm_answer': output['choices'][0]['message']['content'],\n        'output': str(output)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:08:54.724948Z","iopub.execute_input":"2025-07-23T00:08:54.726162Z","iopub.status.idle":"2025-07-23T00:08:54.733345Z","shell.execute_reply.started":"2025-07-23T00:08:54.726131Z","shell.execute_reply":"2025-07-23T00:08:54.732107Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def main(data, output_json_path, num_chunks=2):  # Kaggle-safe parallelism\n    # Load existing results (Kaggle working directory)\n    if os.path.exists(output_json_path):\n        with open(output_json_path, 'r', encoding='utf-8') as f:\n            results = json.load(f)\n    else:\n        results = []\n    \n    # Filter already processed samples\n    processed_ids = {r['id'] for r in results}\n    to_process = [s for s in data if s['id'] not in processed_ids]\n    print(f\"Need to process {len(to_process)} samples split into {num_chunks} chunks\")\n\n    # Split data into chunks\n    chunks = split_into_chunks(to_process, num_chunks)\n\n    # Load model once (shared across threads)\n    print(\"Loading model on Kaggle CPU...\")\n    llm = load_model()\n    # time.sleep(3)  # Let CUDA initialize fully on Kaggle\n\n    # Process chunks in parallel with Kaggle GPU limits\n    with ThreadPoolExecutor(max_workers=num_chunks) as executor:\n        futures = [executor.submit(process_chunk, chunk, llm) for chunk in chunks]\n        \n        # Collect results\n        for future in futures:\n            chunk_results = future.result()\n            results.extend(chunk_results)\n            \n            # Save progress (Kaggle persists /kaggle/working/)\n            with open(output_json_path, 'w', encoding='utf-8') as f:\n                json.dump(results, f, indent=4, ensure_ascii=False)\n            print(f\"Completed chunk. Total processed: {len(results)}\")\n\n    # Final save\n    with open(output_json_path, 'w', encoding='utf-8') as f:\n        json.dump(results, f, indent=4, ensure_ascii=False)\n    print(f\"All done! Total results: {len(results)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:09:01.120155Z","iopub.execute_input":"2025-07-23T00:09:01.121163Z","iopub.status.idle":"2025-07-23T00:09:01.130619Z","shell.execute_reply.started":"2025-07-23T00:09:01.121128Z","shell.execute_reply":"2025-07-23T00:09:01.129456Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def main(data, output_json_path, max_workers=1):  # 1 worker for Kaggle GPU safety\n    # Load existing results (Kaggle saves to /kaggle/working/)\n    if os.path.exists(output_json_path):\n        with open(output_json_path, 'r', encoding='utf-8') as f:\n            results = json.load(f)\n    else:\n        results = []\n    \n    processed_ids = {r['id'] for r in results}\n    to_process = [s for s in data if s['id'] not in processed_ids]\n    print(f\"Need to process {len(to_process)} samples\")\n\n    # Process in parallel with Kaggle constraints\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        futures = {executor.submit(process_sample, sample): sample for sample in to_process}\n\n        for future in as_completed(futures):\n            sample = futures[future]\n            try:\n                result = future.result()\n                results.append(result)\n                print(f\"Processed sample {result['id']}\")\n\n                # Save to Kaggle working directory (persists between sessions)\n                with open(output_json_path, 'w', encoding='utf-8') as f:\n                    json.dump(results, f, indent=4, ensure_ascii=False)\n            except Exception as e:\n                print(f\"Error processing sample {sample['id']}: {e}\")\n                # Save error to results for debugging\n                results.append({\n                    'id': sample['id'],\n                    'error': str(e)\n                })\n                with open(output_json_path, 'w', encoding='utf-8') as f:\n                    json.dump(results, f, indent=4, ensure_ascii=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:09:24.063358Z","iopub.execute_input":"2025-07-23T00:09:24.063734Z","iopub.status.idle":"2025-07-23T00:09:24.074016Z","shell.execute_reply.started":"2025-07-23T00:09:24.063707Z","shell.execute_reply":"2025-07-23T00:09:24.072659Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"data_path = \"/kaggle/input/curebench-data/curebench_testset_phase1.jsonl\"\ndata = []\nwith open(data_path, 'r') as f:\n    for line in f:\n        obj = json.loads(line)  # parse each JSON object from the line\n        data.append(obj)\n\n# Kaggle working directory (where outputs are saved)\noutput_path = \"/kaggle/working/curebench_results.json\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:10:11.844229Z","iopub.execute_input":"2025-07-23T00:10:11.844686Z","iopub.status.idle":"2025-07-23T00:10:11.857430Z","shell.execute_reply.started":"2025-07-23T00:10:11.844656Z","shell.execute_reply":"2025-07-23T00:10:11.856219Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# temp = process_sample(data[42])\n# data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:10:20.363996Z","iopub.execute_input":"2025-07-23T00:10:20.365377Z","iopub.status.idle":"2025-07-23T00:10:20.370425Z","shell.execute_reply.started":"2025-07-23T00:10:20.365316Z","shell.execute_reply":"2025-07-23T00:10:20.368918Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"main(data, output_path, max_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:10:36.834638Z","iopub.execute_input":"2025-07-23T00:10:36.835880Z"}},"outputs":[{"name":"stdout","text":"Need to process 230 samples\n","output_type":"stream"},{"name":"stderr","text":"llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n","output_type":"stream"},{"name":"stdout","text":"Processed sample QBzcFcVzpisx\n","output_type":"stream"},{"name":"stderr","text":"llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n","output_type":"stream"},{"name":"stdout","text":"Processed sample lMCGiMsPyJct\n","output_type":"stream"},{"name":"stderr","text":"llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n","output_type":"stream"},{"name":"stdout","text":"Processed sample OUTPs0m2REUz\n","output_type":"stream"},{"name":"stderr","text":"llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n","output_type":"stream"},{"name":"stdout","text":"Processed sample OhzVvNLVpXly\n","output_type":"stream"},{"name":"stderr","text":"llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nllama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\n# llm = Llama.from_pretrained(\n# \trepo_id=\"mradermacher/TxAgent-T1-Llama-3.1-8B-GGUF\",\n# \tfilename=\"TxAgent-T1-Llama-3.1-8B.Q8_0.gguf\",\n#     # n_ctx=512\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n\n# file_path = '/kaggle/input/curebench-data/curebench_testset_phase1.jsonl'\n# output_json_path = '/kaggle/working/llm_answers_test.json'\n\n# data = []\n# with open(file_path, 'r') as f:\n#     for line in f:\n#         obj = json.loads(line)  # parse each JSON object from the line\n#         data.append(obj)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data[0].keys()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data[0]['question']\n# data[0]['options']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# IDX = 0\n\n# prompt = f\"\"\"Please answer the question: {data[IDX]['question']}. These are the options {data[IDX]['options']}\"\"\"\n\n# messages = [\n#             {\"role\": \"system\", \"content\": \"You are a reasoning clinical assistant.\"},\n#             {\"role\": \"user\", \"content\": prompt}\n#         ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # %%timeit\n# output = llm.create_chat_completion(\n#     messages = messages\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%timeit\n# output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output['choices'][0]['message']['content'].split('[FinalAnswer]')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Load existing results if file exists\n# if os.path.exists(output_json_path):\n#     with open(output_json_path, 'r', encoding='utf-8') as f:\n#         results = json.load(f)\n# else:\n#     results = []\n\n# processed_ids = {r['id'] for r in results}  # track already done samples\n\n# for idx, sample in enumerate(data):\n#     if sample['id'] in processed_ids:\n#         continue  # skip already processed samples\n    \n#     prompt = f\"\"\"Please answer the question: {sample['question']}. These are the options {sample['options']}.\"\"\"\n\n#     messages = [\n#         {\"role\": \"system\", \"content\": \"You are a reasoning clinical assistant.\"},\n#         {\"role\": \"user\", \"content\": prompt}\n#     ]\n\n#     output = llm.create_chat_completion(messages=messages, max_tokens=512)\n#     llm_answer = output['choices'][0]['message']['content']\n\n#     result = {\n#         'id': sample['id'],\n#         'question_type': sample['question_type'],\n#         'question': sample['question'],\n#         'options': sample['options'],\n#         'llm_answer': llm_answer\n#     }\n#     results.append(result)\n\n#     # Write after each processed sample (or every N samples for speed)\n#     with open(output_json_path, 'w', encoding='utf-8') as f:\n#         json.dump(results, f, indent=4, ensure_ascii=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# with open(output_json_path, 'w', encoding='utf-8') as f_json:\n#     json.dump(results, f_json, indent=4, ensure_ascii=False)\n\n# print(f\"Results written to JSON file: {output_json_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python -m pip install --quiet --no-index -v --find-links=/kaggle/input/aimo-packages/offline_packages trl --pre","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python -m pip install --quiet --no-index -v --find-links=/kaggle/input/aimo-packages/offline_packages -U bitsandbytes\n# !python -m pip install --no-index -v --find-links=/kaggle/input/aimo-packages/offline_packages levenshtein","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python -m pip install --quiet --no-index -v --find-links=/kaggle/input/aimo-packages/offline_packages transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python /kaggle/input/curebench-code/run.py --config /kaggle/input/curebench-code/metadata_config_test.json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset,Dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom trl import GRPOConfig, GRPOTrainer\n\nimport datetime\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig,\n    PrinterCallback,\n)\nfrom tqdm import tqdm\nimport torch\nimport time\nimport transformers\nimport pandas as pd\nimport numpy as np\n\ntransformers.set_seed(42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    MAX_TRAIN = 100\n    MAX_TOKENS = 2048\n    NUM_GENERATIONS = 4\n    USE_PEFT = True\n    BATCH_SIZE=1\n    MAX_STEPS = 80\n    \n    BETA = 0.04\n    LR = 1.e-5\n    \n    MODEL_NAME = '/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-1.5b/2'\n    splitter = '<｜Assistant｜>'\n    \n    step_count=10\n    DEBUG = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef extract_boxed_text(text):\n    pattern = r'oxed{(.*?)}'\n    matches = re.findall(pattern, text)\n    if not matches:\n        return \"\"\n    for match in matches[::-1]:\n        if match != \"\":\n            return match\n    return \"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_prompt(sample):\n    question = sample['problem']\n    chat = [{\"role\": \"system\", \"content\": \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.  The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>\"},\n            {\"role\": \"user\", \"content\": question + ' Return final answer within \\\\boxed{}, after taking modulo 1000.'},]\n    sample['prompt'] = tokenizer.apply_chat_template(\n            conversation=chat,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n    return sample","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## We would also want a reward function based on accuracy\n# split after </think>, then get the answer within bbox\n\n## We can also do a reward based on Similarity of \n\nimport re\n\ndef format_reward_func(completions, **kwargs):\n    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n    pattern = r\"^<think>.*?</think>.*?oxed{(.*?)}.*?$\"\n    matches = [re.match(pattern, content, re.DOTALL) for content in completions]\n    return [1.0 if match else 0.0 for match in matches]\n\n\ndef extract_boxed_text(text):\n    pattern = r'oxed{(.*?)}'\n    matches = re.findall(pattern, text)\n    if not matches:\n        return \"\"\n    for match in matches[::-1]:\n        if match != \"\":\n            return match\n    return \"\"\n\ndef accuracy_reward_func(completions, answer, **kwargs):\n    # Regular expression to capture content inside \\boxed{}\n    contents = [extract_boxed_text(completion) for completion in completions]\n    # Reward 1 if the content is the same as the ground truth, 0 otherwise\n    return [1.0 if c == str(gt) else 0.0 for c, gt in zip(contents, answer)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndevice_map = 'auto'\nif CFG.USE_PEFT:\n    compute_dtype = getattr(torch, \"float16\")\n    bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type='nf4',\n            bnb_4bit_compute_dtype=compute_dtype,\n            bnb_4bit_use_double_quant=False,\n        )\n    original_model = AutoModelForCausalLM.from_pretrained(CFG.MODEL_NAME, \n                                                          device_map=device_map,\n                                                          quantization_config=bnb_config,\n                                                          trust_remote_code=True\n                                                         )\n    tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME,\n                                              trust_remote_code=True,\n                                              # padding_side=\"left\"\n                                             )\nelse:\n    original_model = AutoModelForCausalLM.from_pretrained(CFG.MODEL_NAME, \n                                                          device_map=device_map,\n                                                          trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME,\n                                              trust_remote_code=True,\n                                              # padding_side=\"left\"\n                                             )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gen(model, text, max_tokens):\n    model_input = tokenizer(text, return_tensors='pt').to(model.device)\n    model.eval()\n    with torch.no_grad():\n        tok = model.generate(**model_input, max_new_tokens=max_tokens, pad_token_id=tokenizer.pad_token_type_id)\n        outputs = []\n        for i in range(len(tok)):\n            res = tokenizer.decode(tok[i], skip_special_tokens=True)\n            output = res.split(CFG.splitter)[-1]\n            outputs.append(output)\n        return outputs[0] if len(outputs) == 1 else outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_rewards(model, dataset, reward_functions: dict[str, callable], max_tokens: int, num_generations: int):\n    completions = []\n    other_info = []\n    for example in tqdm(dataset):\n        txt = example['prompt']\n        kw = {k: v for k, v in example.items() if k not in {'prompt', 'completion'}}\n        for _ in range(num_generations):\n            other_info.append(kw)\n            \n        completion = gen(model, [txt]*num_generations, max_tokens)\n        if isinstance(completion, str):\n            completions.append(completion)\n        else:\n            completions += completion\n        \n    kwargs = {k: [d[k] for d in other_info] for k in other_info[0].keys()}\n    res = {}\n    for nm, reward_func in reward_functions.items():\n        v = reward_func(completions=completions, **kwargs)\n        print(nm, np.mean(v))\n        res[nm] = np.mean(v)\n    return res","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reward_functions = {'formatting': format_reward_func, 'accuracy': accuracy_reward_func, 'solution_quality': levenshtein_reward_func}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if not CFG.DEBUG:\n#     original_rewards = evaluate_rewards(model=original_model, dataset=dataset['test'], reward_functions=reward_functions, max_tokens=CFG.MAX_TOKENS, num_generations=CFG.NUM_GENERATIONS)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# dtstr = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n# output_directory=f\"./DEEPSEEK-GRPO-{dtstr}\"\n\n\n# training_args = GRPOConfig(\n#     output_dir=output_directory,\n    \n#     learning_rate=CFG.LR,\n    \n#     per_device_train_batch_size=CFG.BATCH_SIZE,\n    \n#     gradient_accumulation_steps=1,\n#     max_steps=CFG.MAX_STEPS,\n    \n#     max_completion_length=CFG.MAX_TOKENS,  #8192\n#     num_generations=CFG.NUM_GENERATIONS,\n#     beta=CFG.BETA,\n    \n#     logging_steps=CFG.step_count,\n#     logging_dir=\"./logs\",\n#     save_strategy=\"steps\",\n#     save_steps=CFG.step_count,\n# #     eval_strategy=\"steps\",\n# #     eval_steps=CFG.step_count,\n# #     do_eval=True,\n#     # gradient_checkpointing=True,  # Will crash the whole thing\n#     report_to=\"none\",\n#     overwrite_output_dir = 'True',    \n# )\n\n# # Will typically use the AdamW optimizer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if CFG.USE_PEFT:\n#     peft_config = LoraConfig(\n#         r=32, #Rank\n#         lora_alpha=32,\n#         target_modules=[\n#             'q_proj',\n#             'k_proj',\n#             'v_proj',\n#             'dense'\n#         ],\n#         bias=\"none\",\n#         lora_dropout=0.05,  # Conventional\n#         task_type=\"CAUSAL_LM\",\n#     )\n#     trainer = GRPOTrainer(\n#         model=original_model,\n#         reward_funcs=list(reward_functions.values()),\n#         args=training_args,\n#         train_dataset=dataset['train'],\n#         peft_config=peft_config,\n#         callbacks=[PrinterCallback()]\n#     )\n# else:\n#     trainer = GRPOTrainer(\n#         model=original_model,\n#         reward_funcs=list(reward_functions.values()),\n#         args=training_args,\n#         train_dataset=dataset['train'],\n#         callbacks=[PrinterCallback()]\n#     )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if CFG.USE_PEFT:\n#     print('Loading trained model')\n#     CHKPT = CFG.MAX_STEPS\n#     adapter_model_name = f'{output_directory}/checkpoint-{CHKPT}/'\n#     new_model = PeftModel.from_pretrained(original_model, adapter_model_name)\n# else:\n#     new_model = original_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# rewards = evaluate_rewards(model=new_model, dataset=dataset['test'], reward_functions=reward_functions, max_tokens=CFG.MAX_TOKENS, num_generations=CFG.NUM_GENERATIONS)\n# rewards","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}